{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(991, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['Article_content']\n",
    "Y = df['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "Y = le.fit_transform(Y.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_RandomForest = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',  RandomForestClassifier(n_estimators=200,oob_score=True,random_state=100,n_jobs=-1,max_features=None,min_samples_leaf=50))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate score for each CV split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores5 = cross_val_score(text_clf_RandomForest, X, Y, cv=10, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997979797979798"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores5.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...imators=200, n_jobs=-1,\n",
       "            oob_score=True, random_state=100, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model using all the data\n",
    "text_clf_RandomForest.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19336"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_clf_RandomForest.named_steps['vect'].get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Articles extracted from RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "# RSS feed url\n",
    "url = 'http://www.dailymirror.lk/RSS_Feeds/breaking-news'\n",
    "feed = feedparser.parse(url)\n",
    "entries = feed['entries']\n",
    "\n",
    "from newspaper import Article\n",
    "articles = []\n",
    "for entry in entries:\n",
    "    if entry:\n",
    "        url = entry['link']\n",
    "        article = Article(url)\n",
    "        article.download()\n",
    "        article.parse()\n",
    "        articles.append({\n",
    "            'Title': entry['title'],\n",
    "            'Date': entry['published'],\n",
    "            'Article_content': article.text,\n",
    "            'Summary': entry['summary'],\n",
    "            'Link': entry['link'],\n",
    "        })\n",
    "import re\n",
    "TAG_RE=re.compile(r'<[^>]+>')\n",
    "for article in articles:\n",
    "    summary = article['Summary']\n",
    "    pure_summary=TAG_RE.sub('',summary)\n",
    "    article['Summary']=pure_summary\n",
    "import csv\n",
    "with open('articles_from_rss.csv',mode='w') as csv_file:\n",
    "    fieldnames=['Title','Date','Article_content','Summary','Link']\n",
    "    writer=csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "\n",
    "    writer.writeheader()\n",
    "    for article in articles:\n",
    "        writer.writerow({'Title':article['Title'].encode(\"utf-8\"),'Date':article['Date'].encode(\"utf-8\"),'Article_content':article['Article_content'].encode(\"utf-8\"),'Summary':article['Summary'].encode(\"utf-8\"),'Link':article['Link'].encode(\"utf-8\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv('articles_from_rss.csv', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles = df_new['Article_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_articles = new_articles.fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "type_arr = text_clf_RandomForest.predict(new_articles)\n",
    "print(type_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Dengue articles to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mysql.connector import MySQLConnection, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  'user': 'root',\n",
    "  'password': '',\n",
    "  'host': '127.0.0.1',\n",
    "  'database': 'dengue_sri_lanka',\n",
    "  'raise_on_warnings': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_article(publication_date, title, content, summary, link):\n",
    "    query = \"INSERT IGNORE INTO news_articles(publication_date, title, content, summary, link) \" \\\n",
    "            \"VALUES(%s,%s,%s,%s,%s)\"\n",
    "    args = (publication_date, title, content, summary, link)\n",
    " \n",
    "    try:\n",
    "        conn = MySQLConnection(**config)\n",
    " \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, args) \n",
    "        conn.commit()\n",
    "    except Error as error:\n",
    "        print(error)\n",
    " \n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract articles with 0 as the result and update the db table (without duplicates)\n",
    "index = 0;\n",
    "for x in type_arr:\n",
    "    if x==0: \n",
    "        insert_article(articles[index]['Date'], articles[index]['Title'], articles[index]['Article_content'], articles[index]['Summary'], articles[index]['Link'])\n",
    "\n",
    "    index = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code for info extraction (use above Dengue articles and extract data from that and store in respective tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from word2number import w2n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_publication_date = re.compile(r\"\"\"(?:\\d{4}-\\d{2}-\\d{2})|(?:\\d{1,2}\\s*(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|April|May|June|July|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s*\\d{4})\"\"\",re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_cases = re.compile(r'(\\d+(?:(?:,|\\s|(?:,\\s)|(?:\\s,))\\d{1,3}){0,1})\\s*(?:dengue\\scases|dengue\\sfever\\scases|cases|suspected\\sdengue\\scases|suspected\\scases|suspected\\sdengue\\sfever\\scases|patients|dengue\\spatients|from)', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_deaths = re.compile(r\"\"\"((?:\\d+)|(?:[^\\S]\\d{1,3}(?:(?:,|\\s|(?:,\\s)|(?:\\s,))\\d+)+)|\n",
    "(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|\n",
    "(?:(?:(?:twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety)\\s*)(?:one|two|three|four|five|six|seven|eight|nine)?)))\n",
    "\\s*\n",
    "(?:deaths|death|dead\\b|(?:people\\s*|patients\\s*)?(?:have\\s*|had\\s*)?(?:died|dead\\b)|dengue(?:\\s|-)related\\sdeath)\n",
    "\"\"\", re.VERBOSE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to extract sentences having no. of cases/deaths\n",
    "regex_for_sentences = re.compile(r\"[^.]*?(?:\\d+(?:(?:,|\\s|(?:,\\s)|(?:\\s,))\\d+)*\\s*(?:dengue\\scases|dengue\\sfever\\scases|cases|suspected\\sdengue\\scases|suspected\\scases|suspected\\sdengue\\sfever\\scases)|(?:(?:(?:\\d+(?:(?:,|\\s|(?:,\\s)|(?:\\s,))\\d+)*)|(?:one|two|three|four|five|six|seven|eight|nine|ten|eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|eighteen|nineteen|(?:(?:(?:twenty|thirty|forty|fifty|sixty|seventy|eighty|ninety)\\s*)(?:one|two|three|four|five|six|seven|eight|nine)?)))\\s*(?:deaths|death|dead\\b|(?:people\\s*|patients\\s*)?(?:have\\s*|had\\s*)?(?:died|dead\\b)|patients|dengue\\srelated\\sdeath)))[^.]*\\.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex_for_years 20xx\n",
    "regex_for_years = re.compile(r\"\"\"\\b20(?:\\d{2}){1}\\b|this\\syear\"\"\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for months\n",
    "regex_for_months = re.compile(r'Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|April|May|June|July|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for \"so far\"\n",
    "regex_for_so_far = re.compile(r'so\\sfar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to catch all time periods\n",
    "regex_for_time_period_of = re.compile(r\"\"\"((?:\\b20(?:\\d{2}){1}\\b|this\\syear)|(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|April|May|June|July|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember))|(?:so\\sfar))\"\"\", re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for districts\n",
    "regex_for_districts = re.compile(r'Jaffna|Kilinochchi|Mannar|Mullaitivu|Vavuniya|Puttalam|Kurunegala|Gampaha|Colombo|Kalutara|Anuradhapura|Polonnaruwa|Matale|Kandy|Nuwara\\sEliya|Kegalle|Ratnapura|Trincomalee|Trinco|Batticaloa|Ampara|Badulla|Monaragala|Hambantota|Matara|Galle', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for provinces\n",
    "regex_for_provinces = re.compile(r'Western|Central|Eastern|North\\sCentral|Northern|North\\sWestern|Sabaragamuwa|Southern|Uva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for countries\n",
    "regex_for_country = re.compile(r'Sri Lanka|island|country|countrywide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for different places\n",
    "regex_for_place_of = re.compile(r'((?:Jaffna|Kilinochchi|Mannar|Mullaitivu|Vavuniya|Puttalam|Kurunegala|Gampaha|Colombo|Kalutara|Anuradhapura|Polonnaruwa|Matale|Kandy|Nuwara\\sEliya|Kegalle|Ratnapura|Trincomalee|Trinco|Batticaloa|Ampara|Badulla|Monaragala|Hambantota|Matara|Galle)|(?:Western|Central|Eastern|North\\sCentral|Northern|North\\sWestern|Sabaragamuwa|Southern|Uva)|(?:Sri Lanka|island|country|countrywide))', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences(text):\n",
    "    return regex_for_sentences.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_dates = re.compile(r\"\"\"(?:\\d{1,2}\\s*(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|April|May|June|July|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\\s*\\d{4})\"\"\",re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_month_in_number(month):\n",
    "    m = {\n",
    "        'jan': 1,\n",
    "        'feb': 2,\n",
    "        'mar': 3,\n",
    "        'apr':4,\n",
    "        'may':5,\n",
    "        'jun':6,\n",
    "        'jul':7,\n",
    "        'aug':8,\n",
    "        'sep':9,\n",
    "        'oct':10,\n",
    "        'nov':11,\n",
    "        'dec':12\n",
    "        }\n",
    "    s = month.strip()[:3].lower()\n",
    "\n",
    "    try:\n",
    "        out = m[s]\n",
    "        return out\n",
    "    except:\n",
    "        raise ValueError('Not a month')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_published_date(text):\n",
    "    dates = regex_for_publication_date.findall(text)\n",
    "    for index,date in enumerate(dates):\n",
    "        if(regex_for_dates.match(date)):\n",
    "            day = re.search('\\d{1,2}', date).group()\n",
    "            month = get_month_in_number(re.search('\\D{3,9}', date).group())\n",
    "            year = re.search('\\d{4}', date).group()\n",
    "            date = year + \"-\" + str(month) + \"-\" + day\n",
    "            dates[index] = date\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cases(text):\n",
    "    cases = regex_for_cases.findall(text)\n",
    "    for index,case in enumerate(cases):\n",
    "        case = case.replace(\" \",\"\")\n",
    "        case = case.replace(\",\",\"\")\n",
    "        case = int(case)   \n",
    "        cases[index] = case\n",
    "    return cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_for_digits = re.compile(r'(?:(?:\\s)*\\d+(?:(?:\\s)\\d+)*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deaths(text):\n",
    "    deaths = regex_for_deaths.findall(text)\n",
    "    for index,death in enumerate(deaths):\n",
    "        if(regex_for_digits.match(death)):\n",
    "            death = death.replace(\" \",\"\")\n",
    "            death = death.replace(\",\",\"\")\n",
    "        death = w2n.word_to_num(death)   \n",
    "        \n",
    "        deaths[index] = death\n",
    "    return deaths        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_years(text,text_date):\n",
    "    years = regex_for_years.findall(text)    \n",
    "    for index,year in enumerate(years):\n",
    "        if(year.lower() == \"this year\"):\n",
    "            pub_date = get_published_date(text_date)[0]\n",
    "            year = re.search('\\d{4}',pub_date).group()\n",
    "            years[index] = year\n",
    "    return years\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_months(text):\n",
    "    return regex_for_months.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_so_far(text):\n",
    "    return regex_for_so_far.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_period_of(text):\n",
    "    return regex_for_time_period_of.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_provinces(text):\n",
    "    return regex_for_provinces.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_districts(text):\n",
    "    return regex_for_districts.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_country(text):\n",
    "    return regex_for_country.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_place_of(text):\n",
    "    return regex_for_place_of.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# row = [published_date, cases, deaths, year, month, time, province, countrywide, district]\n",
    "def get_info(published_date, cases, deaths, time_periods, places):\n",
    "    lst = []\n",
    "    for index,case in enumerate(cases):\n",
    "        row = []\n",
    "        if(len(published_date) != 0):    \n",
    "            row.append(published_date[0])\n",
    "        else:\n",
    "            row.append(\"\")\n",
    "        row.append(case)\n",
    "        if(len(deaths) > index):\n",
    "            row.append(deaths[index])\n",
    "        else:\n",
    "            row.append(0)\n",
    "#         time_period\n",
    "        if(len(time_periods) > index):\n",
    "            if(len(get_years(time_periods[index], published_date[0])) != 0):\n",
    "                row.append(get_years(time_periods[index], published_date[0])[0])\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "            elif(len(get_months(time_periods[index])) != 0):\n",
    "                row.append(\"\")\n",
    "                row.append(get_months(time_periods[index])[0])\n",
    "                row.append(\"\")\n",
    "            else:\n",
    "                row.append(\"\")\n",
    "                row.append(\"\")\n",
    "                row.append(time_periods[index])\n",
    "           \n",
    "        else:\n",
    "            row.append(\"\")   \n",
    "            row.append(\"\")   \n",
    "            row.append(\"\")   \n",
    "\n",
    "#        places\n",
    "        if(len(places) > index):\n",
    "        \n",
    "            if(len(get_provinces(places[index])) != 0):\n",
    "                row.append(get_provinces(places[index])[0])   \n",
    "                row.append(\"\")   \n",
    "                row.append(\"\") \n",
    "            elif(len(get_country(places[index])) != 0):\n",
    "                row.append(\"\")   \n",
    "                row.append(get_country(places[index])[0])   \n",
    "                row.append(\"\")\n",
    "            elif(len(get_districts(places[index])) != 0):\n",
    "                row.append(\"\")   \n",
    "                row.append(\"\")   \n",
    "                row.append(get_districts(places[index])[0]) \n",
    "            else:\n",
    "                row.append(\"\")   \n",
    "                row.append(\"\")   \n",
    "                row.append(\"\")  \n",
    "        \n",
    "#             row.append(places[index])\n",
    "        else:\n",
    "            row.append(\"\")   \n",
    "            row.append(\"\")   \n",
    "            row.append(\"\")  \n",
    "        lst.append(row)\n",
    "    return lst\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert extracted data into table\n",
    "conn = MySQLConnection(**config)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "\n",
    "for index,x in enumerate(type_arr):\n",
    "    if x==0: #if article is Dengue\n",
    "        row = articles[index]\n",
    "\n",
    "        \n",
    "        \n",
    "        for sentence in get_sentences(row['Article_content']):\n",
    "            lst = get_info(get_published_date(row['Date']),get_cases(sentence),get_deaths(sentence),get_time_period_of(sentence),get_place_of(sentence))\n",
    "            for i,l1 in enumerate(lst):\n",
    "#                 print(l1)\n",
    "\n",
    "                query = \"INSERT INTO regex_output(publication_date, cases, deaths, year, month, time_duration, province, countrywide, article_id, district) VALUES(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\n",
    "\n",
    "                args = (l1[0], l1[1], l1[2], l1[3], l1[4], l1[5], l1[6], l1[7], 7, l1[8])\n",
    "\n",
    "                try:\n",
    "                    cursor.execute(query, args)\n",
    "                    conn.commit()\n",
    "\n",
    "                except Error as error:\n",
    "                    print(error)\n",
    "                \n",
    "cursor.close()\n",
    "conn.close()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
